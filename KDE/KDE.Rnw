\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
%\usepackage[portuguese]{babel}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{graphicx}
\usepackage{makeidx}
\usepackage{graphicx}
\usepackage{lmodern}
\usepackage{multicol}
\usepackage{booktabs}
\usepackage{fancyhdr}
\usepackage{hyperref}
\usepackage[usenames]{color}


\begin{document}
\SweaveOpts{concordance=TRUE}
\pagestyle{fancy}
\fancyhf{}
\renewcommand{\headrulewidth}{0.4pt}
\fancyfoot[C]{\thepage}
\renewcommand{\footrulewidth}{0.4pt}
\fancyfoot[C]{\thepage}
\title{\LARGE \bf
 Exercício 9 - Estimação de densidades utilizando KDE}
\author{ Rodrigo Machado Fonseca - 2017002253}
\thispagestyle{fancy}
\fancyhead[C]{Introdução ao Reconhecimento de Padrões - UFMG \\ Belo Horizonte - \today}
\maketitle
\thispagestyle{fancy}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introdução}
   \par Neste trabalho iremos implementar o algoritmo KDE (\textit{Kernel  density estimation}) \ref{KDE}. Em seguida, iremos utilizá-lo para classificar um conjunto de amostras.
   
\section{KDE}
  \label{KDE}
  \par A maioria dos problemas que lidamos não são bem comportados. Onde os modelos normais não se aplicam é possível utilizar modelos não-paramétricos, que é o caso do KDE.
  
  \par O KDE vai realizar a estimativa, por meio da superposição de funções de densidade em cada ponto da amostra.
  
  \par Neste experimento utilizaremos a função de densidade normal como função de kernel:
  
  \begin{equation}
  p(x) = \frac{1}{N}\sum_{i=1}^{N}\frac{1}{h}K(\frac{x-x_i}{h})
  \end{equation}
  
  \begin{equation}
  K(\frac{x-x_i}{h}) = \frac{1}{\sqrt{2\pi}}\cdot exp(-0.5(\frac{x-x_i}{h})^2)
  \end{equation}
  
  \par Para calcular o valor \textit{h} utilizaremos a regra  Silvermann:
  
  \begin{equation}
  h \approx 1.06\sigma N^{-0.2},
  \end{equation}
  
  \par onde $\sigma$ é o desvio padrão da classe e N é o número de amostras.
  
  \par Para o problema multivariado  utilizaremos a seguinte fórmula:
  
  \begin{equation}
  p(x_i) = \frac{1}{N(\sqrt{2\pi h})^N}\Sigma_{j = 1}^{N}e^{-\frac{(x_i - x_j)^2}{2h^2}}
  \label{px}
  \end{equation}
  
  \par a função a seguir condensa o que foi discutido nesta seção.
<<echo=TRUE>>=
rm(list=ls())

pdfKDE <- function(xi, N, x)
{
  sum <- 0
  h <- 1.06 * sd(x) * N^(-1/5)
  for (i in 1:N)
  {
    sum <- sum + exp(-(1/(2*h^2) * (t(x[i, ] - xi) %*% (x[i, ] - xi))))
  }
  p <- (1/(N * (sqrt(2 * pi * h))^N)) * sum
  return (p)
}
@

\section{Metodologia}

  \par Inicialmente, carregamos os dados da base \textit{mlbench.spirals}. Em sequência, foi necessário separar as amostras em treinamento e teste utilizando a técnica de validação cruzada com 10 folds. O processo de treinamento e teste será repetido 10 vezes, de forma que a cada vez utilizaremos um dos 10 folds para o teste e os outros 9 para treinamento. Para cada par treinamento e teste utilizaremos o classificador de bayes.

<<echo=TRUE>>=
bayes_classifier <- function(x_train, y_train, x_test){
  pC1 = (nrow(x_train[y_train == 1, ])) / nrow(x_train)
  pC2 = 1-pC1
  c1 = x_train[y_train == 0, ]
  c2 = x_train[y_train == 1, ]
  y_hat <- c()
  for(i in 1:nrow(x_test)){
    p1 <- pdfKDE(x_test[i, ], nrow(c1),
                 c1)
    p2 <- pdfKDE(x_test[i, ], nrow(c2),
                 c2)
    if(p1*pC1/(p2*pC2) >= 1){
      y_hat <-c(y_hat, 0)
    }
    else{
      y_hat <- c(y_hat, 1)
    }
  }
  return(y_hat)
}
@

\section{Resultados}

  \par A seguir estão apresentados os valores da acurácia obtido para cada iteração, o desvio padrão das acurácias e a média das acurácias, respectivamente. 
<<echo=False>>=
set.seed(12)
library(mlbench)
data <- mlbench.spirals(200, sd = 0.05)
x <- cbind(as.matrix(data[["x"]]))
y <- (as.matrix(as.numeric(data[["classes"]]))-1)
index <- sample(1:nrow(x), length(1:nrow(x)))
accuracy <- matrix(nrow = 10, ncol = 1)
j <- 1
best <- 0
for(i in seq(20, 200, 20)){
  test <- index[(i-19):i]
  train <- index[-index[(i-19):i]]
  x_train <- x[train, ]
  y_train <- y[train, ]
  x_test <- x[test, ]
  y_test <- y[test, ]
  y_hat <- bayes_classifier(x_train, y_train, x_test)
  aux <- sum((y_test == y_hat)*1)/20
  accuracy[j, ] <- aux
  if(aux > best){
    best_train <- train
    best_test <- test
    save_index <- j
    best <- aux 
  }
  j <- j+1
}

print(accuracy)
print(apply(accuracy, 2, sd))
print(apply(accuracy, 2, mean))
@

  \par Para o fold \Sexpr{save_index} iremos plotar os dados de testes no espaço de verossimilhanças, a superfície de densidade de probabilidade, o conjunto de amostras antes do treinamento e após o treinamento, respectivamente.

\begin{figure}[h]
\centering
<<fig = True, echo=false>>=
test <- best_test
train <- best_train
x_train <- x[train, ]
y_train <- y[train, ]
x_test <- x[test, ]
y_test <- y[test, ]

y_hat <- matrix(nrow = length(y_test), ncol = 1)
espaco_de_verossimilhanca <- matrix(nrow = length(y_test), ncol = 2)

pC1 = (nrow(x_train[y_train == 1, ])) / nrow(x_train)
pC2 = 1-pC1
c1 = x_train[y_train == 0, ]
c2 = x_train[y_train == 1, ]
y_hat <- c()
for(i in 1:nrow(x_test)){
    p1 <- pdfKDE(x_test[i, ], nrow(c1),
                 c1)
    p2 <- pdfKDE(x_test[i, ], nrow(c2),
                 c2)
  
  espaco_de_verossimilhanca[i, 1] <- p1
  espaco_de_verossimilhanca[i, 2] <- p2
  
  K = (p1 * pC1) / (p2 * pC2)
  
  y_hat[i] <- if (K >= 1) 0 else 1
}

# Plota teste no espaço de verossimilhança
plot(espaco_de_verossimilhanca[y_hat == 0, 1], espaco_de_verossimilhanca[y_hat == 0, 2], col = 'blue', xlim = c(0,1e-06), ylim = c(0,1e-06), xlab = '', ylab = '')
par(new=T)
plot(espaco_de_verossimilhanca[y_hat == 1, 1], espaco_de_verossimilhanca[y_hat == 1, 2], col = 'red', xlim = c(0,1e-06), ylim = c(0,1e-06), xlab = '', ylab = '')
@
\caption{Dados de teste no espaço de verossimilhanças para o fold \Sexpr{save_index}}
\label{8}
\end{figure}

\begin{figure}[h]
\centering
<<fig = True, echo=False>>=
# Dividir amostras de treino para C1 e C2
x_train_C1 <- x_train[y_train == 0, ]
y_train_C1 <- y_train[y_train == 0]
x_train_C2 <- x_train[y_train == 1, ]
y_train_C2 <- y_train[y_train == 1]

# Superficie de densidade de probabilidade
x1seq <- seq(-2.5,2.5, 0.1)
x2seq <- seq(-2.5,2.5, 0.1)
M1 <- matrix(nrow = length(x1seq), ncol = length(x2seq))
M2 <- matrix(nrow = length(x1seq), ncol = length(x2seq))
for(i in 1:length(x1seq))
{
  for(j in 1:length(x2seq))
  {
    x1 <-x1seq[i]
    x2 <- x2seq[j]
    x_in <-as.vector(cbind(x1, x2))
    pdf_c1 <- pdfKDE(x_in, length(y_train_C1), x_train_C1)
    pdf_c2 <- pdfKDE(x_in, length(y_train_C2), x_train_C2)
    
    M1[i, j] = pdf_c1
    M2[i, j] = pdf_c2
  }  
}

plot(x_train[y_train==1,1], x_train[y_train == 1, 2], col = 'red', xlim = c(-1.5, 1.5), ylim = c(-1.5,1.5), xlab = '', ylab= '')
par(new=T)
plot(x_train[y_train==0,1], x_train[y_train == 0, 2], col = 'blue', xlim = c(-1.5, 1.5), ylim = c(-1.5,1.5), xlab = '', ylab= '')
par(new=T)
contour(x1seq, x2seq, M1, xlim = c(-1.5,1.5), ylim = c(-1.5,1.5), xlab = '', ylab='')
par(new=T)
contour(x1seq, x2seq, M2, xlim = c(-1.5,1.5), ylim = c(-1.5,1.5), xlab = '', ylab='')
@
\caption{Superfície de densidade de probabilidade para o fold \Sexpr{save_index}.}
\label{9}
\end{figure}

\begin{figure}[h]
\centering
<<fig = True, echo=false>>=
# Plota treinamento e teste
plot(x_train[y_train==1,1], x_train[y_train == 1, 2], col = 'red', xlim = c(-1.5, 1.5), ylim = c(-1.5,1.5), xlab = '', ylab= '')
par(new=T)
plot(x_train[y_train==0,1], x_train[y_train == 0, 2], col = 'blue', xlim = c(-1.5, 1.5), ylim = c(-1.5,1.5), xlab = '', ylab= '')
par(new=T)
plot(x_test[,1], x_test[, 2], col = 'green', xlim = c(-1.5, 1.5), ylim = c(-1.5,1.5), xlab = '', ylab= '')
@
\caption{Amostras antes da classificação para o fold \Sexpr{save_index}.}
\label{10}
\end{figure}

\begin{figure}[h]
\centering
<<fig = True, echo=false>>=
# Plota treinamento e teste
plot(x_train[y_train==1,1], x_train[y_train == 1, 2], col = 'red', xlim = c(-1.5, 1.5), ylim = c(-1.5,1.5), xlab = '', ylab= '')
par(new=T)
plot(x_train[y_train==0,1], x_train[y_train == 0, 2], col = 'blue', xlim = c(-1.5, 1.5), ylim = c(-1.5,1.5), xlab = '', ylab= '')
par(new=T)
plot(x_test[y_test==0,1], x_test[y_test==0, 2], col = 'green', xlim = c(-1.5, 1.5), ylim = c(-1.5,1.5), xlab = '', ylab= '')
par(new=T)
plot(x_test[y_test==1,1], x_test[y_test==1, 2], col = 'black', xlim = c(-1.5, 1.5), ylim = c(-1.5,1.5), xlab = '', ylab= '')
@
\caption{Amostras após a classificação para o fold \Sexpr{save_index}. Em pretro as amostras classificadas como vermelho e em verde as amostras classificadas com azul. }
\label{10}
\end{figure}

\section{Discussão}

  \par Os gráficos das amostras antes e depois da ilustração foram interessantes para ilustrar a capacidade de classificação do nosso algoritmo. Como a espiral está muito bem comportada, somos capazes de ver se a classificação ocorreu conforme o esperado.
  
  \par Além disso, com o gráfico da superfície de densidade de probabilidade podemos observar os contornos das funções de densidade de verossimilhanças estimadas pelo método KDE. Podemos notar que esses contornos seguem o formato espiral das duas classes e resultam na separação linear das classes, como pode ser observado no gráfico dos dados de teste no espaço das verossimilhanças. Nota-se, portanto, que a técnica adotada fez com que um problema de separação não-linear no espaço dos atributos se tornasse linear no espaço das verossimilhanças, o que possibilita a sua classificação.
  
   \par Com o experimento foi possível compreender melhor o funcionamento do classificador de Bayes com o KDE e implementá-lo de forma satisfatória. Os resultados obtidos podem provar que o classificador de Bayes com o KDE mostrou-se muito eficiente para esse conjunto de dados. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}